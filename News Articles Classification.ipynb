{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words from data and fill in the article and labels lists\n",
    "\n",
    "articles = []\n",
    "labels = []\n",
    "\n",
    "with open(\"/Users/alikazmi/Desktop/bbc-text.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        article = row[1]\n",
    "        for word in stop_words:\n",
    "            token = ' ' + word + ' '\n",
    "            article = article.replace(token, ' ')\n",
    "            article = article.replace(' ', ' ')\n",
    "        articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameters required to build and train the model.\n",
    "\n",
    "vocab_size = 5000 # make the top list of words (common words)\n",
    "embedding_dim = 64\n",
    "max_length = 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>' # OOV = Out of Vocabulary\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Training and Testing Set\n",
    "\n",
    "train_size = int(len(articles) * training_portion)\n",
    "\n",
    "train_articles = articles[0: train_size]\n",
    "train_labels = labels[0: train_size]\n",
    "\n",
    "test_articles = articles[train_size:]\n",
    "test_labels = labels[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size 1780\n",
      "train_articles 1780\n",
      "train_labels 1780\n",
      "test_articles 445\n",
      "test_labels 445\n"
     ]
    }
   ],
   "source": [
    "print(\"train_size\", train_size)\n",
    "print(f\"train_articles {len(train_articles)}\")\n",
    "print(\"train_labels\", len(train_labels))\n",
    "print(\"test_articles\", len(test_articles))\n",
    "print(\"test_labels\", len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization, Sequencing and Padding\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_articles)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_articles)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create labels\n",
    "\n",
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "test_label_seq = np.array(label_tokenizer.texts_to_sequences(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          320000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 386,822\n",
      "Trainable params: 386,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Create the Model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(embedding_dim)))\n",
    "model.add(Dense(6, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56/56 - 6s - loss: 1.5849 - accuracy: 0.3107 - val_loss: 1.3481 - val_accuracy: 0.3775\n",
      "Epoch 2/10\n",
      "56/56 - 5s - loss: 1.2430 - accuracy: 0.5213 - val_loss: 0.8115 - val_accuracy: 0.7685\n",
      "Epoch 3/10\n",
      "56/56 - 5s - loss: 0.6894 - accuracy: 0.7944 - val_loss: 1.1697 - val_accuracy: 0.6517\n",
      "Epoch 4/10\n",
      "56/56 - 5s - loss: 0.5651 - accuracy: 0.8517 - val_loss: 0.4854 - val_accuracy: 0.8494\n",
      "Epoch 5/10\n",
      "56/56 - 5s - loss: 0.3777 - accuracy: 0.8775 - val_loss: 0.5309 - val_accuracy: 0.8067\n",
      "Epoch 6/10\n",
      "56/56 - 6s - loss: 0.3371 - accuracy: 0.8820 - val_loss: 0.4008 - val_accuracy: 0.9079\n",
      "Epoch 7/10\n",
      "56/56 - 5s - loss: 0.1601 - accuracy: 0.9697 - val_loss: 0.3196 - val_accuracy: 0.9303\n",
      "Epoch 8/10\n",
      "56/56 - 6s - loss: 0.0975 - accuracy: 0.9831 - val_loss: 0.3195 - val_accuracy: 0.9079\n",
      "Epoch 9/10\n",
      "56/56 - 6s - loss: 0.0609 - accuracy: 0.9899 - val_loss: 0.2347 - val_accuracy: 0.9438\n",
      "Epoch 10/10\n",
      "56/56 - 5s - loss: 0.0385 - accuracy: 0.9933 - val_loss: 0.2135 - val_accuracy: 0.9416\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "\n",
    "num_epochs = 10\n",
    "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(test_padded, test_label_seq), verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.9621271e-04 9.7422421e-01 6.6736010e-03 5.8249221e-04 9.7991928e-05\n",
      "  1.8025404e-02]]\n",
      "1\n",
      "sport\n"
     ]
    }
   ],
   "source": [
    "txt = [\"Ryerson's basketball team won against Hamilton Warriors on Sunday, July 3rd 2020.\"]\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(txt)\n",
    "padded = pad_sequences(seq, maxlen=max_length)\n",
    "pred = model.predict(padded)\n",
    "labels = ['sport', 'bussiness', 'politics', 'tech', 'entertainment'] \n",
    "\n",
    "print(pred)\n",
    "print(np.argmax(pred))\n",
    "print(labels[np.argmax(pred)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
